{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample contains the following keys:\n",
      "- sdf\n",
      "- grasps\n",
      "- scores\n",
      "\n",
      "Shapes:\n",
      "sdf: torch.Size([48, 48, 48])\n",
      "grasps: torch.Size([480, 7])\n",
      "scores: torch.Size([480])\n",
      "\n",
      "Basic statistics:\n",
      "sdf:\n",
      "  Min: -0.7169\n",
      "  Max: 1.4683\n",
      "  Mean: 0.6416\n",
      "  Std: 0.2782\n",
      "\n",
      "grasps:\n",
      "  Min: -0.5236\n",
      "  Max: 1.8326\n",
      "  Mean: 0.1521\n",
      "  Std: 0.4945\n",
      "\n",
      "scores:\n",
      "  Min: -1.5000\n",
      "  Max: 8.6819\n",
      "  Mean: 1.6106\n",
      "  Std: 2.8319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from dataset import GraspDataset\n",
    "\n",
    "# Create dataset\n",
    "data_path = Path('data/processed')\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "# Get a single sample\n",
    "sample = dataset[0]\n",
    "\n",
    "# Print available keys\n",
    "print(\"Sample contains the following keys:\")\n",
    "for key in sample.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Print tensor shapes and data types\n",
    "print(\"\\nShapes:\")\n",
    "for key, tensor in sample.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "\n",
    "# Basic statistics for numerical tensors\n",
    "print(\"\\nBasic statistics:\")\n",
    "for key, tensor in sample.items():\n",
    "    if torch.is_floating_point(tensor):\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Min: {tensor.min().item():.4f}\")\n",
    "        print(f\"  Max: {tensor.max().item():.4f}\")\n",
    "        print(f\"  Mean: {tensor.mean().item():.4f}\")\n",
    "        print(f\"  Std: {tensor.std().item():.4f}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Overfitting on 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataset import GraspDataset\n",
    "from model import GQEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing GQEstimator\n",
      "Input size: 48\n",
      "Flattened size: 3456\n",
      "Number of parameters: 1218977\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GQEstimator(\n",
    "    input_size=48,\n",
    "    base_channels=16,\n",
    "    fc_dims=[256, 128, 64]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Get a small number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "\tgrasps:  [0.03288976103067398, -0.4235876500606537, 0.20093415677547455, 0.1622592806816101, -0.16071856021881104, 0.5709341764450073, -0.14416876435279846]\n",
      "\tscore:  1.9573743343353271\n",
      "Sample 1:\n",
      "\tgrasps:  [-0.04639378935098648, 0.005759271793067455, 0.6709341406822205, -0.1295223981142044, -0.170878067612648, 0.1809341162443161, -0.008676999248564243]\n",
      "\tscore:  0.929709792137146\n",
      "Sample 2:\n",
      "\tgrasps:  [0.028589002788066864, -0.4741508364677429, 0.17093412578105927, 0.15106919407844543, -0.15743368864059448, 0.590934157371521, -0.1308029145002365]\n",
      "\tscore:  -1.0003485679626465\n",
      "torch.Size([7])\n",
      "torch.Size([])\n",
      "torch.Size([48, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "data_path = Path('data/processed')\n",
    "\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "scene = dataset[0]\n",
    "\n",
    "def get_samples(scene, num_samples):\n",
    "    # Get num_samples many unique indices\n",
    "    indices = torch.randperm(len(scene['grasps']))[:num_samples]\n",
    "    samples = []\n",
    "    for i in indices:\n",
    "        sample = {\n",
    "            'grasps': scene['grasps'][i].float().to(device),\n",
    "            'scores': scene['scores'][i].float().to(device),\n",
    "            'sdf': scene['sdf'].float().to(device)\n",
    "        }\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "samples = get_samples(scene, 10)\n",
    "\n",
    "for i, sample in enumerate(samples[:3]):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"\\tgrasps: \", sample['grasps'].tolist())\n",
    "    print(\"\\tscore: \", sample['scores'].item())\n",
    "\n",
    "\n",
    "print(samples[0]['grasps'].shape)\n",
    "print(samples[0]['scores'].shape)\n",
    "print(samples[0]['sdf'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Overfit on these samples from that one scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3062205954274075, 2.303736984165835, 2.302465810423564, 2.301415713092865, 2.3005105132937387, 2.299713099715882, 2.298960475145941, 2.298289549509718, 2.2976979652841694, 2.2970939746337535, 2.2965100186047493, 2.2959228638675997, 2.2953585123774247, 2.29475023838022, 2.2941662255034316, 2.293588230610476, 2.2929771291906946, 2.292390081868507, 2.2916325486585265, 2.2911155466397757, 2.2905059232056373, 2.2898534661100713, 2.2892342002363875, 2.288565125921741, 2.288072717335308, 2.287418888596585, 2.2866880498826503, 2.2859612220316192, 2.285212161601521, 2.284426288632676, 2.2837580088060347, 2.2830702054081486, 2.2823960325215014, 2.2815190626308324, 2.280671987158712, 2.2792434289352967, 2.277911917399615, 2.2768316663103176, 2.2759363509481774, 2.2746728191152217, 2.2737953047966584, 2.2727465993259104, 2.2719775449484585, 2.270829304866493, 2.2696633111685514, 2.2685999399982393, 2.2676730139181016, 2.2665162940043957, 2.265627229679376, 2.264767000777647, 2.263927267305553, 2.263026043958962, 2.262125295586884, 2.2609921550378203, 2.2600417473353445, 2.259104035887867, 2.258301328867674, 2.2572086989879607, 2.2565631115809084, 2.2555790481157603, 2.2545254925265907, 2.253768542595208, 2.253225067257881, 2.252104515582323, 2.251430452801287, 2.250849398039281, 2.2498148757964374, 2.2489458601921797, 2.248077270947397, 2.247481308504939, 2.2467643251642584, 2.2455873427912594, 2.244955803826451, 2.244457628764212, 2.2435877365991472, 2.242071762494743, 2.2414687050506474, 2.240210662968457, 2.2395943513140084, 2.2385521959513426, 2.2377442851662637, 2.2369615353643892, 2.2365028869360684, 2.235737830772996, 2.2351093167439102, 2.234232645668089, 2.2339030794799326, 2.233330924808979, 2.2325365144759415, 2.2320066140964627, 2.2314977893605827, 2.230935860797763, 2.230437244847417, 2.23002139814198, 2.2294409370049832, 2.228879452869296, 2.227992131188512, 2.2278141733258963, 2.227434591203928, 2.2268700249493123, 2.225816270709038, 2.225517952442169, 2.2251396790146827, 2.2247861113399265, 2.224424209445715, 2.223909628391266, 2.222981199249625, 2.2223101291805505, 2.222071747481823, 2.2214672338217496, 2.220849234610796, 2.220368866249919, 2.219590352475643, 2.2192379135638474, 2.2190595347434283, 2.2185154661536215, 2.217767881974578, 2.217150454223156, 2.216774419695139, 2.216009694710374, 2.2155028585344554, 2.2149569530040027, 2.2145052455365657, 2.2138438008725645, 2.21343397051096, 2.2130570292472838, 2.212229546159506, 2.211800366640091, 2.2113539196550844, 2.210774765536189, 2.21000221259892, 2.209628514945507, 2.2092445459216834, 2.2084888935089113, 2.207942547649145, 2.2072936918586494, 2.206984292715788, 2.206372232362628, 2.2054289042949677, 2.2046774450689552, 2.204411073774099, 2.2037034455686806, 2.203077771142125, 2.202758014947176, 2.2019040796905758, 2.201168302819133, 2.2008052982389925, 2.2001550395041702, 2.1991927795112134, 2.198854351043701, 2.19815731048584, 2.1974030610173942, 2.196741019189358, 2.1961108591407537, 2.1955423802137375, 2.1947946842759847, 2.194236595928669, 2.19320147074759, 2.1928583070635796, 2.1921529717743398, 2.1915682654827835, 2.1906754087656735, 2.190189668163657, 2.1892956595867874, 2.18866822719574, 2.1881272051483394, 2.1872771624475718, 2.186325677111745, 2.1859047822654247, 2.185226933658123, 2.184599316120148, 2.1838638223707676, 2.1830265503376722, 2.181843946501613, 2.181220119819045, 2.1806961860507728, 2.1796880401670933, 2.1792347475886347, 2.1784797977656125, 2.1777852434664964, 2.176988795399666, 2.175949598476291, 2.1751918844878673, 2.174631744250655, 2.1737376131117343, 2.1729241270571946, 2.1719726018607615, 2.1711774341762067, 2.170544693619013, 2.1697065211832522, 2.1687418960034845, 2.168139786273241, 2.1668800689280032, 2.166144870221615, 2.165600852295756, 2.1642378687858583, 2.163468001410365, 2.1629863519221546, 2.162040226906538, 2.160837712883949, 2.159808214753866, 2.159488870948553, 2.158596607670188, 2.1575450841337442, 2.156704876944423, 2.155821620672941, 2.1551270209252835, 2.153705507516861, 2.153014937788248, 2.1521969590336085, 2.150968823954463, 2.1498872708529233, 2.1490872405469417, 2.1479350958019494, 2.147202866896987, 2.1462908390909434, 2.1454499669373037, 2.1443581193685533, 2.14342425391078, 2.1422320757061244, 2.141281521320343, 2.140568370372057, 2.139680317044258, 2.138575414568186, 2.1374704595655203, 2.13656568415463, 2.135432864353061, 2.134174769744277, 2.133272946998477, 2.1323347337543965, 2.131450517848134, 2.1300653111189605, 2.1291090816259386, 2.1284040585160255, 2.127186968550086, 2.1262180019170045, 2.124415470659733, 2.1239544678479434, 2.123444255068898, 2.121496786549687, 2.1207426849752666, 2.11988710090518, 2.1174997959285973, 2.117017151415348, 2.1168031454086305, 2.1150996312499046, 2.1139167748391627, 2.112781169638038, 2.1113345094025133, 2.109795469418168, 2.1090887408703565, 2.1071453355252743, 2.1066418658941983, 2.1059488121420147, 2.104251716285944, 2.102736534550786, 2.101898258179426, 2.100511222705245, 2.0995051763951778, 2.0980192966759206, 2.096957303211093, 2.0956085335463284, 2.0945169910788537, 2.093359700962901, 2.091640494763851, 2.0907380271703007, 2.089929662644863, 2.088134540617466, 2.0868570163846014, 2.085338646173477, 2.084530123695731, 2.083469422534108, 2.0816251650452613, 2.080639246851206, 2.078730583935976, 2.077717423439026, 2.076585841551423, 2.0747785534709693, 2.0739659905433654, 2.0728856660425663, 2.0709724605083464, 2.069612017273903, 2.0683660943061115, 2.0673331756144764, 2.0657149758189917, 2.064522521197796, 2.063032107055187, 2.0616554386913775, 2.060216818749905, 2.0587300434708595, 2.0567700674757363, 2.0559842901304366, 2.0550713818520308, 2.052988039702177, 2.051709604263306, 2.0499870859086515, 2.049191451072693, 2.0478150926530363, 2.045640392228961, 2.0445040084421633, 2.0433043144643306, 2.0412257010117174, 2.0401033367961645, 2.0391890462487936, 2.036569537036121, 2.035936283133924, 2.0348572758957744, 2.033214837126434, 2.0309872273355722, 2.0297397427260875, 2.028679692000151, 2.026675611361861, 2.0255822902545333, 2.0234228629618882, 2.0223165046423675, 2.0203946409747005, 2.019423328153789, 2.0182878993451596, 2.0158153297379613, 2.014139636605978, 2.0133909922093154, 2.0107243549078704, 2.0096025267615913, 2.0087104784324765, 2.007162963412702, 2.0041634567081927, 2.003589758463204, 2.003267586603761, 2.000379853323102, 1.9988346349447965, 1.9969206362962724, 1.9949394892901182, 1.9938546450808645, 1.9927067203447222, 1.9894236985594034, 1.9890355426818132, 1.9887596948072315, 1.9863036775961518, 1.9838751129806043, 1.9817073494195938, 1.980465890094638, 1.9785397214815021, 1.9762553848326205, 1.9730812503024935, 1.9715358726680279, 1.970144476182759, 1.9683249333873392, 1.9668628808110953, 1.964004373922944, 1.9625115228816867, 1.9609529625624418, 1.9595483023673297, 1.9556083351373672, 1.955713326111436, 1.9560315273702145, 1.9529044687747956, 1.949473936855793, 1.948344485089183, 1.9460498616099358, 1.9448352022096516, 1.9433332161977888, 1.9400538183748721, 1.939323532767594, 1.9377325877547265, 1.9359629968181253, 1.93324107658118, 1.9319704692810773, 1.9306242881342768, 1.9270295910537243, 1.926430070772767, 1.9254641508683563, 1.921799797937274, 1.9206671386957168, 1.9201091857627035, 1.9162995614111424, 1.9148877125233412, 1.9146736450493336, 1.9119130924344063, 1.9086497800424695, 1.907578223757446, 1.9051890779286622, 1.9032534062862396, 1.9023245895281433, 1.8981827735900878, 1.8984632581472396, 1.8980076344683767, 1.8950804255902767, 1.8916359951719641, 1.8905332101508976, 1.8890460498631, 1.885785348713398, 1.8849205812439322, 1.8838761696591972, 1.8800689974799751, 1.8789741521701218, 1.8784407503902911, 1.8735167445614933, 1.873271334543824, 1.8736511467024684, 1.869599823281169, 1.867735591158271, 1.8658760527148843, 1.8633180188015104, 1.8623335702344774, 1.8585225595161319, 1.8577845549210905, 1.8575751926749944, 1.854093612730503, 1.8507215386256575, 1.850401059538126, 1.8472150241956116, 1.8463588425889612, 1.8456510240212083, 1.8407776413485408, 1.8398753751069308, 1.8395958477631211, 1.8355872372165323, 1.8344991303980351, 1.8334737194702029, 1.8289657462388278, 1.828141143731773, 1.8276118787005544, 1.8239922182634474, 1.8219934741035104, 1.8207370487973094, 1.8183618273586035, 1.815171191841364, 1.8143458301201463, 1.8119453167542816, 1.8105561524629592, 1.8081539813429117, 1.8063073134049774, 1.803986687771976, 1.8023267053067684, 1.7984248977154493, 1.7983213683590293, 1.7984192185103893, 1.7939682889729738, 1.792070822045207, 1.7915299775078892, 1.786618172377348, 1.786252353619784, 1.785996792279184, 1.7815860405564308, 1.7804541144520045, 1.7786925841122865, 1.7769501571543516, 1.7737595886923372, 1.7721828929148615, 1.7702252347022296, 1.768111929576844, 1.7647982727736236, 1.7641744996421038, 1.7618581881746649, 1.75964240655303, 1.7581829622387886, 1.7540240199305117, 1.7537761454470455, 1.7531603890471161, 1.7495020788162947, 1.7478337702341378, 1.7451557378284632, 1.7437831562012434, 1.7406784754246474, 1.739493843447417, 1.737303590402007, 1.735315659129992, 1.731261440459639, 1.7313033627346157, 1.7308509660884739, 1.7263170984108, 1.7249159008730204, 1.7250050424598158, 1.7203191235661506, 1.718830129224807, 1.7184977085329591, 1.714264255715534, 1.712743289070204, 1.7119183879345656, 1.7075775161385536, 1.7067306881770492, 1.7064037441043183, 1.7014930852223187, 1.7005649874918163, 1.7001189078437164, 1.6950531087117269, 1.6946942870505155, 1.6947850891621783, 1.6904505318030716, 1.6883316491381266, 1.6876908149919472, 1.682525205682032, 1.6824033583921847, 1.6825773216725792, 1.6782379401032812, 1.676214126474224, 1.675086476758588, 1.670728271396365, 1.6696297003771179, 1.668849372002296, 1.6633992829010822, 1.6643792266899253, 1.663950738805579, 1.6587816608662251, 1.6576973827774055, 1.6569831442975556, 1.6522590637381653, 1.652171730379115, 1.6518416403101583, 1.6469713603553828, 1.6458114817029128, 1.6462310587361118, 1.6423154100441024, 1.6398770600819261, 1.6371105994803656, 1.6355835111034822, 1.631804091495269, 1.6316039928526151, 1.6299311102113279, 1.6267560945900186, 1.6256949283175346, 1.6223929627357392, 1.6212960403739998, 1.6186391057040055, 1.6173508841357944, 1.6140800133070343, 1.6134858183460892, 1.6113714261393965, 1.6092859378933781, 1.6052901565599313, 1.6053965297833201, 1.6043172322126338, 1.5976040057750651, 1.6008038600324652, 1.600965087559598, 1.5947598681930686, 1.5933766379137524, 1.5934990962239681, 1.58926532821788, 1.5880213474098128, 1.587187942743185, 1.5823883478646166, 1.5814722627052107, 1.5809447973500936, 1.576163690647809, 1.575520292657893, 1.5744505769514945, 1.570428972353693, 1.5697447823942638, 1.566736824729014, 1.5658845472382381, 1.5649227156536654, 1.5577512024203315, 1.5623195624910295, 1.5634861356811598, 1.5566504519898445, 1.5543215394718572, 1.5540385560831056, 1.5513346506631933, 1.5474631401244552, 1.5464027162175626, 1.5452152768149972, 1.5399518094491214, 1.5407725774217398, 1.5406569960992784, 1.534749139426276, 1.5348945376928895, 1.5351606150623411, 1.5310008134692907, 1.5290183170232923, 1.528074546996504, 1.5240209898445756, 1.5232657945714891, 1.5222293410915881, 1.5156446339096874, 1.5195105028338731, 1.519029125664383, 1.5129270842298865, 1.512421357166022, 1.513199704280123, 1.5094555930234492, 1.5067853216081857, 1.5044726164545863, 1.5022488334216177, 1.5000863787718117, 1.4976810192223637, 1.496737149823457, 1.4915370240341872, 1.4947482849005609, 1.4936028738506137, 1.488787461956963, 1.4873995364177972, 1.4865660386625676, 1.4810748171759769, 1.482751749549061, 1.482694058213383, 1.4772312140790747, 1.476394220860675, 1.4761217240709812, 1.4732961445348338, 1.4707242730655707, 1.4686079059028998, 1.461934225820005, 1.4681556063937022, 1.4673447979614138, 1.4596936186542735, 1.4609524027211591, 1.4615699795074761, 1.457036407245323, 1.4545066013466568, 1.4530188084405382, 1.451353438460501, 1.4481738744565518, 1.44679245603038, 1.4431874834612244, 1.4433792125317269, 1.4427846042672172, 1.4368751723965942, 1.4398143518250435, 1.4395496133249253, 1.4328134394135303, 1.433155021618586, 1.433778918668395, 1.4307791440805886, 1.4275588183416403, 1.4257242466457682, 1.423685148601726, 1.421174323183368, 1.4197493027444579, 1.4168228426817222, 1.416051587281709, 1.4119435519271064, 1.413461644286872, 1.4127914477247032, 1.4072108198073692, 1.4081426657496194, 1.4079968646716225, 1.402806648949627, 1.4032452222256324, 1.4031144143438723, 1.4000140159390866, 1.397056024777703, 1.3954420170746744, 1.392143610015046, 1.3917243053379935, 1.38823190984549, 1.388629994308576, 1.387468008324504, 1.3831433151150123, 1.383619254414225, 1.3825170588330367, 1.378533318452537, 1.3785618141293525, 1.3778851629700513, 1.3728402964305133, 1.3742926604521926, 1.3737105047097429, 1.3684461120981724, 1.3685170932207256, 1.3685188974486664, 1.3654227488208561, 1.3629353772383184, 1.3618630242533982, 1.3582577526103705, 1.3579995635896922, 1.3572162016760558, 1.3523403472499922, 1.354540890431963, 1.3536347458604723, 1.3481846056180076, 1.3496225485345348, 1.349158506002277, 1.3468834713101387, 1.3440538976807148, 1.3422070365864784, 1.3402602262794971, 1.3369771558325738, 1.3362718721851707, 1.3348206216935068, 1.3317318243905902, 1.3311483980156482, 1.3293571901973338, 1.327543487632647, 1.326141403382644, 1.3240700266789645, 1.3226880730129777, 1.3178585915244185, 1.3238764364330564, 1.3213423469103873, 1.315435760631226, 1.3181153067445848, 1.317295170761645, 1.314603718649596, 1.3115340368822217, 1.3093404824845494, 1.307703212555498, 1.3046679358929396, 1.3038505457807332, 1.302186028799042, 1.2995570032857358, 1.2987313543446362, 1.2969960602931678, 1.2951036772225053, 1.293892899202183, 1.2890167951001785, 1.294831559463637, 1.2919161652214826, 1.28676209137775, 1.2895004900638014, 1.2880890986416489, 1.2850722128525376, 1.2825893819797785, 1.2811575174331664, 1.2795156181324274, 1.277166664879769, 1.275509883230552, 1.2739768684376032, 1.2705344232497737, 1.2704692520899699, 1.2678599315229804, 1.2671638208441436, 1.2651348769664765, 1.2640297275036574, 1.260921332007274, 1.2608531433157624, 1.2589430042076857, 1.2580080166924745, 1.256653597857803, 1.254379550088197, 1.2529016583459451, 1.25172144244425, 1.2471335095178802, 1.2535226608539234, 1.2497945294715465, 1.2445949988788925, 1.2485805700474883, 1.2463123469613493, 1.244176651723683, 1.241080367844552, 1.2397067996673286, 1.2380963442847133, 1.2363271673675627, 1.2343039417173713, 1.2327828105539083, 1.229100625263527, 1.2301786840660498, 1.2283396306447685, 1.2261503322748468, 1.2250221811234951, 1.223716569133103, 1.2207465558545665, 1.2201777938520535, 1.2177319759037346, 1.217714851303026, 1.2166806010529398, 1.2125661455036607, 1.2159271652868484, 1.213401428796351, 1.2094339888193644, 1.2107827113650274, 1.2094370942562818, 1.207708733296022, 1.205562344752252, 1.203849096200429, 1.201893276674673, 1.2005054283421486, 1.198850012291223, 1.1952131320605985, 1.196541600173805, 1.193920445535332, 1.1932127393316478, 1.191297659603879, 1.190101767028682, 1.188149818708189, 1.1868501224787906, 1.184896278800443, 1.183810791373253, 1.1801518765660148, 1.185568726141355, 1.1816184105351568, 1.1779862283845433, 1.181480663955881, 1.178181282663718, 1.1772644698619843, 1.1745893779676408, 1.1728833701694383, 1.1711755977943539, 1.1697172610089184, 1.168156785517931, 1.1656624316470698, 1.1647835394600405, 1.1632817439502106, 1.1611310398555361, 1.1603398924693464, 1.1576101249433122, 1.1580685290624388, 1.1564854271244258, 1.1555785226402804, 1.1532124309567735, 1.1523889707867057, 1.1499720018007793, 1.1496649130480363, 1.1471714500105008, 1.1471511903801002, 1.1460692737717182, 1.142827003831917, 1.1452783175103831, 1.1428831001743673, 1.1405239471874666, 1.141318746181787, 1.139338825829327, 1.1385996266733855, 1.1366025317227468, 1.134692445024848, 1.1331061176490038, 1.1304570073843934, 1.1296775644877926, 1.128334015631117, 1.1273486668709665, 1.1256592562887817, 1.1245467226486654, 1.121388074933202, 1.122553294757381, 1.1205393896671012, 1.1201777162496, 1.1184804702177644, 1.1156990903895347, 1.1157175192958675, 1.1137480333214627, 1.1133007288677619, 1.1117977439193054, 1.1104551730444654, 1.1088332545128652, 1.1073826058767735, 1.1046420214464887, 1.1051993966219016, 1.1030784938309808, 1.1033107032591942, 1.1018368300050496, 1.0999538558302446, 1.0995788984524553, 1.0977801840053871, 1.097084542992525, 1.095365770929493, 1.0940601531183347, 1.0914023635777994, 1.0921037972264458, 1.0899114443105646, 1.0895613705040887, 1.087919318350032, 1.0869994096923619, 1.0849353443132714, 1.0839760888484307, 1.0816084284277168, 1.0814173691673203, 1.0796110379276798, 1.0794705132371747, 1.0775109119538684, 1.0770539298537187, 1.0751413482823409, 1.0742986560449936, 1.073165253107436, 1.0710318228276592, 1.0728554828907364, 1.0702335311099886, 1.0699149246443995, 1.0685209207935258, 1.0665208261925727, 1.065579903521575, 1.064369585888926, 1.0630440864246338, 1.0618377928971312, 1.060661358665675, 1.0589698284398765, 1.059111156646395, 1.0631600691063796, 1.056168181169778, 1.0566081296667107, 1.0568640403150327, 1.0532167391153053, 1.054088403028436, 1.0519833199330606, 1.0497787197236903, 1.0484862230950966, 1.0469051941996441, 1.0454266614513472, 1.0431592136039398, 1.043138464540243, 1.040634011637303, 1.0411991955712439, 1.0399749110452832, 1.0388328869550605, 1.0381591624580324, 1.0366545589640737, 1.035731279157335, 1.0349405544809998, 1.0333859328646213, 1.0323857639436027, 1.031729619964608, 1.0300562831340359, 1.0303496524691582, 1.0287048247293569, 1.026817045349162, 1.025565359008033, 1.024118895153515, 1.0232029902283102, 1.0215346551209223, 1.0206456445448566, 1.0189696098095737, 1.0232831905748754, 1.0180561877321452, 1.0199438875788474, 1.018044407245179, 1.0156376966275276, 1.0163862462155522, 1.0147087408928201, 1.012655090878252, 1.010969528829446, 1.0096783166052774, 1.0083517026680056, 1.0095514836713846, 1.0063836056506261, 1.0079877143143676, 1.0059120483114383, 1.0040025300113484, 1.0021730099979322, 1.0028940194497409, 1.0001568404026329, 1.001229142898228, 0.9992386746453121, 0.9986975697102025, 0.9971599125303328, 0.9964593463926577, 0.9950856652707444, 0.9944980116648366, 0.9925895191903692, 0.9929810134111904, 0.9917897084960714, 0.9907506116200239, 0.9892777566681616, 0.9872625442847494, 0.9883058002887992, 0.986885379278101, 0.9875498195149703, 0.9856024264590815, 0.9838151555508375, 0.9837529086740687, 0.9821400046290364, 0.980504758504685, 0.9796983295411337, 0.9817460717307768, 0.9774434966617264, 0.9796588356839493, 0.9776371215819382, 0.9760494780435692, 0.9750805508461781, 0.9739519578483851, 0.9727448457313586, 0.972094131578342, 0.9717063283082098, 0.9705477726762183, 0.9698624103883049, 0.9685334097186569, 0.967728788106615, 0.9675863030039181, 0.9657368219690398, 0.9668030723172706, 0.9647930302162422, 0.9638272552285343, 0.9627905402507168, 0.9617212017386919, 0.9616748675409326, 0.95931063175085, 0.9607646017044317, 0.9594379018759355, 0.9581314154496795, 0.956968439593038, 0.9558189006813336, 0.9560446821182268, 0.9548117427795659, 0.9539569826796651, 0.9528572109062224, 0.952520162665951, 0.9499972418649122, 0.951505846116197, 0.9502387520391494, 0.9507930337334983, 0.9493147525528911, 0.9474832555570174, 0.9467209202703089, 0.9468951150620342, 0.9438161172904074, 0.9463530347493361, 0.9439104233766556, 0.9423911037447397, 0.9423928304313449, 0.9406420139304827, 0.9403634625254199, 0.9429822581878398, 0.9371978416340425]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "losses = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for sample in samples:\n",
    "        optimizer.zero_grad()\n",
    "        sdf_features = model.encode_sdf(sample['sdf'])\n",
    "        flattened_features = torch.cat([sdf_features, sample['grasps']])\n",
    "        pred_quality = model(flattened_features).squeeze()\n",
    "        loss = criterion(pred_quality, sample['scores'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(samples)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "print(losses)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Full Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset samples: 15547, Calculated train size: 12437, Calculated val size: 3110\n",
      "Train dataset size: 100, Validation dataset size: 10\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "val_split = 0.2\n",
    "num_samples = len(dataset)\n",
    "train_size = int(num_samples * (1 - val_split))\n",
    "val_size = num_samples - train_size\n",
    "\n",
    "print(f\"Subset samples: {num_samples}, Calculated train size: {train_size}, Calculated val size: {val_size}\")\n",
    "\n",
    "# Shuffle indices\n",
    "random.seed(42)\n",
    "indices = list(range(num_samples))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split indices\n",
    "train_indices = indices[:100]\n",
    "val_indices = indices[-10:]\n",
    "\n",
    "# Create Subsets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total scenes: 15547, Train scenes: 12437, Val scenes: 3110\n",
      "Train dataset size: 100, Validation dataset size: 10\n",
      "Initializing GQEstimator\n",
      "Input size: 48\n",
      "Flattened size: 864\n",
      "Number of parameters: 46889\n",
      "\n",
      "Starting training for 100 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Training: 1500it [00:13, 109.22it/s]\n",
      "Epoch 1/100 Validation: 150it [00:03, 43.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 39.3668, Val Loss: 61.6466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 Training: 1500it [00:10, 141.68it/s]\n",
      "Epoch 2/100 Validation: 150it [00:00, 361.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 37.5586, Val Loss: 64.6404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Training: 1500it [00:11, 132.45it/s]\n",
      "Epoch 3/100 Validation: 150it [00:00, 353.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 43.1828, Val Loss: 61.3789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 Training: 464it [00:03, 130.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_quality, score_batch)\n\u001b[1;32m     88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 89\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m GRASP_BATCH_SIZE\n\u001b[1;32m     92\u001b[0m num_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m GRASP_BATCH_SIZE\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/optim/adam.py:525\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    523\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    527\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from dataset import GraspDataset, GraspBatchIterableDataset\n",
    "from model import GQEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100\n",
    "VAL_SPLIT = 0.2\n",
    "BASE_CHANNELS = 4\n",
    "FC_DIMS = [32, 16, 8]\n",
    "SCENE_BATCH_SIZE = 1 # Process one scene at a time. Increase if you have lots of system RAM.\n",
    "GRASP_BATCH_SIZE = 480 # Process all 480 grasps per scene at once. Reduce if you run out of VRAM.\n",
    "NUM_WORKERS = 4\n",
    "GRASP_BATCH_SIZE = 32\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Dataset and Dataloaders ---\n",
    "data_path = Path('data/processed')\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "num_samples = len(dataset)\n",
    "train_size = int(num_samples * (1 - VAL_SPLIT))\n",
    "val_size = num_samples - train_size\n",
    "print(f\"Total scenes: {num_samples}, Train scenes: {train_size}, Val scenes: {val_size}\")\n",
    "\n",
    "indices = list(range(num_samples))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:100]\n",
    "val_indices = indices[-10:]\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)\n",
    "print(f\"Train dataset size: {len(train_set)}, Validation dataset size: {len(val_set)}\")\n",
    "\n",
    "# Create the iterable datasets\n",
    "train_grasp_dataset = GraspBatchIterableDataset(train_set, grasp_batch_size=GRASP_BATCH_SIZE, shuffle_scenes=True)\n",
    "val_grasp_dataset = GraspBatchIterableDataset(val_set, grasp_batch_size=GRASP_BATCH_SIZE, shuffle_scenes=False)\n",
    "\n",
    "# The dataloader now yields your desired batches directly!\n",
    "# batch_size=None is important for iterable datasets that do their own batching.\n",
    "pin_memory = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_grasp_dataset, batch_size=None, num_workers=NUM_WORKERS, pin_memory=pin_memory, persistent_workers=True)\n",
    "val_loader = DataLoader(val_grasp_dataset, batch_size=None, num_workers=NUM_WORKERS, pin_memory=pin_memory, persistent_workers=True)\n",
    "\n",
    "# --- Model, Optimizer, Loss ---\n",
    "model = GQEstimator(input_size=48, base_channels=BASE_CHANNELS, fc_dims=FC_DIMS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    num_steps = 0\n",
    "\n",
    "    for sdf, grasp_batch, score_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Training\"):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move to device\n",
    "        sdf = sdf.to(device)\n",
    "        grasp_batch = grasp_batch.to(device)\n",
    "        score_batch = score_batch.to(device)\n",
    "\n",
    "        # 1. Encode SDF\n",
    "        sdf_features = model.encode_sdf(sdf)\n",
    "\n",
    "        # 2. Expand features for the grasp batch\n",
    "        expanded_sdf_features = sdf_features.expand(GRASP_BATCH_SIZE, -1)\n",
    "\n",
    "        # 3. Concatenate features\n",
    "        flattened_features = torch.cat([expanded_sdf_features, grasp_batch], dim=1)\n",
    "\n",
    "        # 4. Predict grasp quality and compute loss\n",
    "        pred_quality = model(flattened_features)\n",
    "        loss = criterion(pred_quality, score_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * GRASP_BATCH_SIZE\n",
    "        num_steps += GRASP_BATCH_SIZE\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_steps\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    num_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for sdf, grasp_batch, score_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Validation\"):\n",
    "            # Move to device\n",
    "            sdf = sdf.to(device)\n",
    "            grasp_batch = grasp_batch.to(device)\n",
    "            score_batch = score_batch.to(device)\n",
    "\n",
    "            # 1. Encode SDF\n",
    "            sdf_features = model.encode_sdf(sdf)\n",
    "\n",
    "            # 2. Expand features for the grasp batch\n",
    "            expanded_sdf_features = sdf_features.expand(GRASP_BATCH_SIZE, -1)\n",
    "\n",
    "            # 3. Concatenate features\n",
    "            flattened_features = torch.cat([expanded_sdf_features, grasp_batch], dim=1)\n",
    "\n",
    "            # 4. Predict grasp quality and compute loss\n",
    "            pred_quality = model(flattened_features)\n",
    "            loss = criterion(pred_quality, score_batch)\n",
    "\n",
    "            total_val_loss += loss.item() * GRASP_BATCH_SIZE\n",
    "            num_steps += GRASP_BATCH_SIZE\n",
    "\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_steps\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# --- Save Model ---\n",
    "model_path = \"model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved successfully to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
