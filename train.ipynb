{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample contains the following keys:\n",
      "- sdf\n",
      "- grasp\n",
      "- score\n",
      "- scene_idx\n",
      "- grasp_idx\n",
      "\n",
      "Shapes:\n",
      "sdf: torch.Size([48, 48, 48])\n",
      "grasp: torch.Size([19])\n",
      "score: torch.Size([])\n",
      "\n",
      "Basic statistics:\n",
      "sdf:\n",
      "  Min: -0.7169\n",
      "  Max: 1.4683\n",
      "  Mean: 0.6416\n",
      "  Std: 0.2782\n",
      "\n",
      "grasp:\n",
      "  Min: -0.2937\n",
      "  Max: 1.8255\n",
      "  Mean: 0.3404\n",
      "  Std: 0.5473\n",
      "\n",
      "Score: 0.33304598927497864\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from dataset import GraspDataset\n",
    "\n",
    "# Create dataset\n",
    "data_path = Path('data/processed')\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "# Get a single sample\n",
    "sample = dataset[1]\n",
    "\n",
    "# Print available keys\n",
    "print(\"Sample contains the following keys:\")\n",
    "for key in sample.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Print tensor shapes and data types\n",
    "print(\"\\nShapes:\")\n",
    "for key, tensor in sample.items():\n",
    "    if key == \"scene_idx\" or key == \"grasp_idx\":\n",
    "        continue\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "\n",
    "# Basic statistics for numerical tensors\n",
    "print(\"\\nBasic statistics:\")\n",
    "for key, tensor in sample.items():\n",
    "    if key == \"scene_idx\" or key == \"grasp_idx\" or key == \"score\":\n",
    "        continue\n",
    "    if torch.is_floating_point(tensor):\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Min: {tensor.min().item():.4f}\")\n",
    "        print(f\"  Max: {tensor.max().item():.4f}\")\n",
    "        print(f\"  Mean: {tensor.mean().item():.4f}\")\n",
    "        print(f\"  Std: {tensor.std().item():.4f}\")\n",
    "        print()\n",
    "\n",
    "print(f\"Score: {sample['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Overfitting on 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataset import GraspDataset\n",
    "from model import GQEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing GQEstimator\n",
      "Input size: 48\n",
      "Flattened size: 3456\n",
      "Number of parameters: 1222049\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GQEstimator(\n",
    "    input_size=48,\n",
    "    base_channels=16,\n",
    "    fc_dims=[256, 128, 64]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Get a small number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset):  7462560\n",
      "Scene indices:  tensor([6434734, 1415756, 6454909, 3504589, 4936646, 2858591, 4715063, 6732764,\n",
      "        5602664, 7244770])\n",
      "Sample 0:\n",
      "\tgrasp:  [-0.17773474752902985, -0.13397440314292908, 0.11592119932174683, 0.10246924310922623, 0.748285174369812, 0.3893835246562958, 0.527209460735321, 0.4079841077327728, 1.50093412399292, 1.8254671096801758, 0.2915802299976349, -0.05906585976481438, 0.393803209066391, 0.0885569304227829, 1.3209340572357178, -0.008675945922732353, 0.21476003527641296, -0.20906585454940796, 0.3854670822620392]\n",
      "\tscore:  0.45679938793182373\n",
      "Sample 1:\n",
      "\tgrasp:  [-0.10872942209243774, -0.022228188812732697, 0.25476232171058655, 0.3320675194263458, 0.9113627672195435, 0.11923542618751526, 0.2119717001914978, 0.2713175415992737, 1.50093412399292, 1.8254671096801758, 0.20595623552799225, -0.1890658587217331, 0.33976852893829346, -0.11889589577913284, -0.08906585723161697, 0.15351073443889618, -0.16655132174491882, 0.42093414068222046, -0.12737423181533813]\n",
      "\tscore:  5.232858657836914\n",
      "Sample 2:\n",
      "\tgrasp:  [0.06636134535074234, 0.02331632375717163, 0.31798219680786133, -0.5675224661827087, 0.811441957950592, 0.02817128784954548, -0.1366988718509674, 0.07300719618797302, 0.1809341162443161, -0.04240916669368744, -0.09966494888067245, 0.340934157371521, -0.12491502612829208, -0.25544098019599915, 0.33093416690826416, -0.087473064661026, -0.1944223791360855, 0.6609341502189636, 0.0361107736825943]\n",
      "\tscore:  0.8850183486938477\n",
      "torch.Size([19])\n",
      "torch.Size([])\n",
      "torch.Size([48, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "data_path = Path('data/processed')\n",
    "\n",
    "dataset = GraspDataset(data_path)\n",
    "print(\"len(dataset): \", len(dataset))\n",
    "\n",
    "def get_samples(dataset, num_samples):\n",
    "    # Get num_samples many unique indices\n",
    "    indices = torch.randperm(len(dataset))[:num_samples]\n",
    "    samples = []\n",
    "    print(\"Scene indices: \", indices)\n",
    "    for i in indices:\n",
    "        # Choose random grasp from scene\n",
    "        sample = dataset[i]\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "samples = get_samples(dataset, 10)\n",
    "\n",
    "for i, sample in enumerate(samples[:3]):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"\\tgrasp: \", sample['grasp'].tolist())\n",
    "    print(\"\\tscore: \", sample['score'].item())\n",
    "\n",
    "\n",
    "print(samples[0]['grasp'].shape)\n",
    "print(samples[0]['score'].shape)\n",
    "print(samples[0]['sdf'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Overfit on these samples from that one scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.213095445930957, 30.17613944709301, 30.140928127244116, 30.10498683191836, 30.064901734143497, 30.019593432545662, 29.96413327381015, 29.88636036515236, 29.787068662047385, 29.661893146671353, 29.498111535608768, 29.275163830723614, 28.945327473036013, 28.47462726091617, 27.815033997036515, 26.928245482733473, 25.780516917724164, 24.40930692161164, 22.89796670973301, 21.432868725061418, 20.199880182743073, 19.313104772567748, 18.779938650131225, 18.515143358707427, 18.408401930332182, 18.374997633695603, 18.365033304691316, 18.359399539232253, 18.353754663467406, 18.346991288661957, 18.33936430811882, 18.331372332572936, 18.323292607069014, 18.31518428325653, 18.30724799633026, 18.29951393008232, 18.292058396339417, 18.2846337556839, 18.277375549077988, 18.270353263616563, 18.263527697324754, 18.256849122047424, 18.25041260123253, 18.244199448823927, 18.23817797899246, 18.232317131757735, 18.226603639125823, 18.22102197408676, 18.215558129549027, 18.210184347629546, 18.20489979982376, 18.199667972326278, 18.194468402862547, 18.189287459850313, 18.184168255329134, 18.179127675294875, 18.17417487502098, 18.16928870677948, 18.164458006620407, 18.159678822755815, 18.154955750703813, 18.150248295068742, 18.145589733123778, 18.140987527370452, 18.136435478925705, 18.13193335533142, 18.127438801527024, 18.12298321723938, 18.11853905916214, 18.11411754488945, 18.10970057249069, 18.1052965760231, 18.100916677713393, 18.096573334932327, 18.092257291078568, 18.08798853158951, 18.083727264404295, 18.079496490955353, 18.07526650428772, 18.071055257320403, 18.066822504997255, 18.06260039806366, 18.058412832021713, 18.054260861873626, 18.050108230113985, 18.046003377437593, 18.04189539551735, 18.03778401017189, 18.03371388912201, 18.029661428928375, 18.025612086057663, 18.021559077501298, 18.01752347946167, 18.01349871754646, 18.00947562456131, 18.005526489019395, 18.001540219783784, 17.997588837146758, 17.993641591072084, 17.989761453866958]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Play around with the learning rate and run this cell multiple times \n",
    "# to see how it affects the loss curve\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    epoch_loss = 0.0\n",
    "    for sample in samples:\n",
    "        optimizer.zero_grad()\n",
    "        sdf_features = model.encode_sdf(sample['sdf'])\n",
    "        flattened_features = torch.cat([sdf_features, sample['grasp']])\n",
    "        pred_quality = model(flattened_features).squeeze()\n",
    "        loss = criterion(pred_quality, sample['score'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(samples)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "print(losses)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Full Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset samples: 7462560, Calculated train size: 5970048, Calculated val size: 1492512\n",
      "Train dataset size: 1000, Validation dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "val_split = 0.2\n",
    "num_samples = len(dataset)\n",
    "train_size = int(num_samples * (1 - val_split))\n",
    "val_size = num_samples - train_size\n",
    "\n",
    "print(f\"Subset samples: {num_samples}, Calculated train size: {train_size}, Calculated val size: {val_size}\")\n",
    "\n",
    "# Shuffle indices\n",
    "random.seed(42)\n",
    "indices = list(range(num_samples))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split indices\n",
    "train_indices = indices[:1000]\n",
    "val_indices = indices[-100:]\n",
    "\n",
    "# Create Subsets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total scenes: 7462560, Train scenes: 5970048, Val scenes: 1492512\n",
      "Train dataset size: 5000, Validation dataset size: 1000\n",
      "Initializing GQEstimator\n",
      "Input size: 48\n",
      "Flattened size: 864\n",
      "Number of parameters: 46873\n",
      "\n",
      "Starting training for 100 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Training: 100%|██████████| 157/157 [00:48<00:00,  3.23it/s]\n",
      "Epoch 1/100 Validation: 100%|██████████| 32/32 [00:07<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 49.9691, Val Loss: 48.7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 Training: 100%|██████████| 157/157 [00:44<00:00,  3.53it/s]\n",
      "Epoch 2/100 Validation: 100%|██████████| 32/32 [00:04<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 43.0461, Val Loss: 42.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Training: 100%|██████████| 157/157 [00:44<00:00,  3.53it/s]\n",
      "Epoch 3/100 Validation:  56%|█████▋    | 18/32 [00:02<00:01,  7.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m actual_batch_size \u001b[38;5;241m=\u001b[39m sdf_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# 1. Encode SDF\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m sdf_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_sdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdf_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# 2. No need to expand - sdf_features is already (B, flattened_size)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# 3. Concatenate features\u001b[39;00m\n\u001b[1;32m    113\u001b[0m flattened_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sdf_features, grasp_batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/grasp-quality/model.py:71\u001b[0m, in \u001b[0;36mGQEstimator.encode_sdf\u001b[0;34m(self, sdf)\u001b[0m\n\u001b[1;32m     68\u001b[0m     single_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Get features (B, channel_dim, D, D, D)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Flatten (B, channel_dim, D, D, D) -> (B, flattened_size)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/nn/modules/conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    719\u001b[0m     )\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from dataset import GraspDataset\n",
    "from model import GQEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 100\n",
    "VAL_SPLIT = 0.2\n",
    "BASE_CHANNELS = 4\n",
    "FC_DIMS = [32, 8]\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Dataset and Dataloaders ---\n",
    "data_path = Path('data/processed')\n",
    "dataset = GraspDataset(data_path)\n",
    "\n",
    "num_samples = len(dataset)\n",
    "train_size = int(num_samples * (1 - VAL_SPLIT))\n",
    "val_size = num_samples - train_size\n",
    "print(f\"Total scenes: {num_samples}, Train scenes: {train_size}, Val scenes: {val_size}\")\n",
    "\n",
    "indices = list(range(num_samples))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:5000]\n",
    "val_indices = indices[-1000:]\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)\n",
    "print(f\"Train dataset size: {len(train_set)}, Validation dataset size: {len(val_set)}\")\n",
    "\n",
    "# The dataloader now yields your desired batches directly!\n",
    "# batch_size=None is important for iterable datasets that do their own batching.\n",
    "pin_memory = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=pin_memory, persistent_workers=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=pin_memory, persistent_workers=True)\n",
    "\n",
    "# --- Model, Optimizer, Loss ---\n",
    "model = GQEstimator(input_size=48, base_channels=BASE_CHANNELS, fc_dims=FC_DIMS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    num_steps = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move to device\n",
    "        sdf_batch = batch['sdf'].to(device)\n",
    "        grasp_batch = batch['grasp'].to(device)\n",
    "        score_batch = batch['score'].to(device)\n",
    "\n",
    "        # Get the actual batch size (handles variable batch sizes)\n",
    "        actual_batch_size = sdf_batch.size(0)\n",
    "\n",
    "        # 1. Encode SDF\n",
    "        sdf_features = model.encode_sdf(sdf_batch)\n",
    "\n",
    "        # 2. No need to expand - sdf_features is already (B, flattened_size)\n",
    "        # 3. Concatenate features\n",
    "        flattened_features = torch.cat([sdf_features, grasp_batch], dim=1)\n",
    "\n",
    "        # 4. Predict grasp quality and compute loss\n",
    "        pred_quality = model(flattened_features)\n",
    "        loss = criterion(pred_quality, score_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * actual_batch_size\n",
    "        num_steps += actual_batch_size\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_steps\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    num_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Validation\"):\n",
    "            sdf_batch = batch['sdf'].to(device)\n",
    "            grasp_batch = batch['grasp'].to(device)\n",
    "            score_batch = batch['score'].to(device)\n",
    "\n",
    "            # Get the actual batch size (handles variable batch sizes)\n",
    "            actual_batch_size = sdf_batch.size(0)\n",
    "\n",
    "            # 1. Encode SDF\n",
    "            sdf_features = model.encode_sdf(sdf_batch)\n",
    "\n",
    "            # 2. No need to expand - sdf_features is already (B, flattened_size)\n",
    "            # 3. Concatenate features\n",
    "            flattened_features = torch.cat([sdf_features, grasp_batch], dim=1)\n",
    "\n",
    "            # 4. Predict grasp quality and compute loss\n",
    "            pred_quality = model(flattened_features)\n",
    "            loss = criterion(pred_quality, score_batch)\n",
    "\n",
    "            total_val_loss += loss.item() * actual_batch_size\n",
    "            num_steps += actual_batch_size\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_steps\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# --- Save Model ---\n",
    "model_path = \"model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved successfully to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total samples: 7462560\n",
      "Train dataset size: 5000, Validation dataset size: 1000\n",
      "Initializing GQEstimator\n",
      "Input size: 48\n",
      "Flattened size: 864\n",
      "Number of parameters: 47145\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "=== Epoch 1/100 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 309\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping triggered at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 196\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Collect unique scene indices for this epoch\u001b[39;00m\n\u001b[1;32m    195\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 196\u001b[0m train_unique_scenes \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_unique_scene_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m val_unique_scenes \u001b[38;5;241m=\u001b[39m collect_unique_scene_indices(val_loader)\n\u001b[1;32m    198\u001b[0m collection_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[8], line 54\u001b[0m, in \u001b[0;36mcollect_unique_scene_indices\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mCollect all unique scene indices that will be used in this epoch.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    set: Set of unique scene indices\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m unique_scenes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     55\u001b[0m     scene_indices \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscene_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     56\u001b[0m     unique_scenes\u001b[38;5;241m.\u001b[39mupdate(scene_indices\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Developer/grasp-quality/dataset.py:150\u001b[0m, in \u001b[0;36mOptimizedGraspDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    149\u001b[0m     scene_idx, grasp_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrasp_locations[idx]\n\u001b[0;32m--> 150\u001b[0m     scene \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_scene\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Handle IndexError by selecting a random valid grasp\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     num_grasps \u001b[38;5;241m=\u001b[39m scene[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Developer/grasp-quality/dataset.py:132\u001b[0m, in \u001b[0;36mOptimizedGraspDataset._load_scene\u001b[0;34m(self, scene_idx)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scene_cache[key]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files[scene_idx]) \u001b[38;5;28;01mas\u001b[39;00m scene_data:\n\u001b[1;32m    131\u001b[0m     scene \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msdf\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mscene_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasps\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(scene_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrasps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(scene_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    135\u001b[0m     }\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scene_cache[scene_idx] \u001b[38;5;241m=\u001b[39m scene\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scene\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:254\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/numpy/lib/format.py:851\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    849\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    850\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 851\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    853\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/site-packages/numpy/lib/format.py:986\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 986\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/zipfile.py:927\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 927\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/adlr/lib/python3.10/zipfile.py:1003\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1002\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1003\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataset import OptimizedGraspDataset\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Training parameters\n",
    "        self.lr = 1e-4\n",
    "        self.epochs = 100\n",
    "        self.train_size = 5000\n",
    "        self.val_size = 1000\n",
    "        self.base_channels = 4\n",
    "        self.fc_dims = [32, 16]\n",
    "        self.batch_size = 32\n",
    "        self.num_workers = 0\n",
    "        self.data_path = 'data/processed'\n",
    "        self.weight_decay = 1e-4\n",
    "        self.early_stopping_patience = 15\n",
    "        self.check_data_distribution = False\n",
    "        \n",
    "        # WandB parameters\n",
    "        self.wandb_entity = 'tairo'\n",
    "        self.project_name = 'adlr'\n",
    "        self.run_name = None\n",
    "\n",
    "# Create args object\n",
    "args = Args()\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0.1):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "def collect_unique_scene_indices(dataloader):\n",
    "    \"\"\"\n",
    "    Collect all unique scene indices that will be used in this epoch.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: DataLoader that yields batches with scene_idx\n",
    "    \n",
    "    Returns:\n",
    "        set: Set of unique scene indices\n",
    "    \"\"\"\n",
    "    unique_scenes = set()\n",
    "    for batch in dataloader:\n",
    "        scene_indices = batch['scene_idx']\n",
    "        unique_scenes.update(scene_indices.tolist())\n",
    "    return unique_scenes\n",
    "\n",
    "def preprocess_epoch_sdfs(unique_scene_indices, dataset, model, device):\n",
    "    \"\"\"\n",
    "    Pre-encode all unique SDFs for the epoch.\n",
    "    \n",
    "    Args:\n",
    "        unique_scene_indices: Set of unique scene indices\n",
    "        dataset: Dataset to get SDFs from\n",
    "        model: Model with encode_sdf method\n",
    "        device: Device to move tensors to\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from scene_idx to encoded SDF features\n",
    "    \"\"\"\n",
    "    sdf_features_cache = {}\n",
    "    \n",
    "    print(f\"Pre-encoding {len(unique_scene_indices)} unique SDFs for this epoch...\")\n",
    "    with torch.no_grad():  # No gradients needed for SDF encoding\n",
    "        for scene_idx in tqdm(unique_scene_indices, desc=\"Encoding SDFs\"):\n",
    "            sdf = dataset.get_sdf(scene_idx).to(device)\n",
    "            sdf_features = model.encode_sdf(sdf)\n",
    "            sdf_features_cache[scene_idx] = sdf_features.detach()  # Store on GPU\n",
    "    \n",
    "    return sdf_features_cache\n",
    "\n",
    "def process_batch_with_cached_features(batch, sdf_features_cache, device):\n",
    "    \"\"\"\n",
    "    Process batch using pre-computed SDF features.\n",
    "    \n",
    "    Args:\n",
    "        batch: Batch from dataloader\n",
    "        sdf_features_cache: Pre-computed SDF features\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (sdf_features_batch, grasp_batch, score_batch)\n",
    "    \"\"\"\n",
    "    scene_indices = batch['scene_idx']\n",
    "    grasp_batch = batch['grasp'].to(device)\n",
    "    score_batch = batch['score'].to(device)\n",
    "    \n",
    "    # Build batch of SDF features by lookup\n",
    "    batch_size = len(scene_indices)\n",
    "    feature_dim = next(iter(sdf_features_cache.values())).shape[0]\n",
    "    sdf_features_batch = torch.zeros(batch_size, feature_dim, device=device)\n",
    "    \n",
    "    for i, scene_idx in enumerate(scene_indices):\n",
    "        sdf_features_batch[i] = sdf_features_cache[scene_idx.item()]\n",
    "    \n",
    "    return sdf_features_batch, grasp_batch, score_batch\n",
    "\n",
    "def main(args):\n",
    "    # --- Device Setup ---\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Dataset and Dataloaders ---\n",
    "    data_path = Path(args.data_path)\n",
    "    dataset = OptimizedGraspDataset(data_path)\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "    print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "    # Create indices and shuffle\n",
    "    indices = list(range(num_samples))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_indices = indices[:args.train_size]\n",
    "    val_indices = indices[-args.val_size:]\n",
    "\n",
    "    train_set = Subset(dataset, train_indices)\n",
    "    val_set = Subset(dataset, val_indices)\n",
    "    print(f\"Train dataset size: {len(train_set)}, Validation dataset size: {len(val_set)}\")\n",
    "\n",
    "    if args.check_data_distribution:\n",
    "        # Data distribution analysis\n",
    "        print(\"\\n=== Data Distribution Analysis ===\")\n",
    "        train_scores = []\n",
    "        val_scores = []\n",
    "\n",
    "        for i in range(len(train_set)):\n",
    "            train_scores.append(train_set[i]['score'].item())\n",
    "            \n",
    "        for i in range(len(val_set)):\n",
    "            val_scores.append(val_set[i]['score'].item())\n",
    "\n",
    "        train_scores = np.array(train_scores)\n",
    "        val_scores = np.array(val_scores)\n",
    "\n",
    "        print(f\"Train scores - Mean: {train_scores.mean():.3f}, Std: {train_scores.std():.3f}\")\n",
    "        print(f\"Val scores   - Mean: {val_scores.mean():.3f}, Std: {val_scores.std():.3f}\")\n",
    "        print(f\"Distribution difference: {abs(train_scores.mean() - val_scores.mean()):.3f}\")\n",
    "        print(\"===================\\n\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    pin_memory = torch.cuda.is_available()\n",
    "    train_loader = DataLoader(\n",
    "        train_set, \n",
    "        batch_size=args.batch_size, \n",
    "        num_workers=args.num_workers, \n",
    "        pin_memory=pin_memory, \n",
    "        persistent_workers=True if args.num_workers > 0 else False,\n",
    "        shuffle=True  # Important: shuffle for training\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, \n",
    "        batch_size=args.batch_size, \n",
    "        num_workers=args.num_workers, \n",
    "        pin_memory=pin_memory, \n",
    "        persistent_workers=True if args.num_workers > 0 else False,\n",
    "        shuffle=False  # No shuffle for validation\n",
    "    )\n",
    "\n",
    "    # --- Model, Optimizer, Loss ---\n",
    "    model = GQEstimator(input_size=48, base_channels=args.base_channels, fc_dims=args.fc_dims).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.7, patience=10\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=args.early_stopping_patience)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(f\"Starting training for {args.epochs} epochs...\")\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        # === EPOCH-LEVEL SDF PREPROCESSING ===\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{args.epochs} ===\")\n",
    "        \n",
    "        # Collect unique scene indices for this epoch\n",
    "        start_time = time.time()\n",
    "        train_unique_scenes = collect_unique_scene_indices(train_loader)\n",
    "        val_unique_scenes = collect_unique_scene_indices(val_loader)\n",
    "        collection_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Train unique scenes: {len(train_unique_scenes)}\")\n",
    "        print(f\"Validation unique scenes: {len(val_unique_scenes)}\")\n",
    "        print(f\"Scene collection time: {collection_time:.2f}s\")\n",
    "        \n",
    "        # Pre-encode all unique SDFs\n",
    "        start_time = time.time()\n",
    "        train_sdf_cache = preprocess_epoch_sdfs(train_unique_scenes, dataset, model, device)\n",
    "        val_sdf_cache = preprocess_epoch_sdfs(val_unique_scenes, dataset, model, device)\n",
    "        preprocessing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"SDF preprocessing time: {preprocessing_time:.2f}s\")\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        total_train_samples = len(train_set)\n",
    "        total_val_samples = len(val_set)\n",
    "        train_efficiency = len(train_unique_scenes) / total_train_samples\n",
    "        val_efficiency = len(val_unique_scenes) / total_val_samples\n",
    "        \n",
    "        print(f\"Train efficiency: {train_efficiency:.3f} (lower is better)\")\n",
    "        print(f\"Val efficiency: {val_efficiency:.3f} (lower is better)\")\n",
    "        print(f\"Train SDF reuse factor: {total_train_samples / len(train_unique_scenes):.1f}x\")\n",
    "        print(f\"Val SDF reuse factor: {total_val_samples / len(val_unique_scenes):.1f}x\")\n",
    "        \n",
    "        # === TRAINING ===\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_steps = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs} Training\")\n",
    "        for batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Fast batch processing with cached SDF features\n",
    "            sdf_features_batch, grasp_batch, score_batch = process_batch_with_cached_features(\n",
    "                batch, train_sdf_cache, device\n",
    "            )\n",
    "\n",
    "            actual_batch_size = sdf_features_batch.size(0)\n",
    "\n",
    "            # Concatenate and predict\n",
    "            flattened_features = torch.cat([sdf_features_batch, grasp_batch], dim=1)\n",
    "            pred_quality = model(flattened_features)\n",
    "            loss = criterion(pred_quality, score_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * actual_batch_size\n",
    "            num_steps += actual_batch_size\n",
    "            \n",
    "            # Update progress bar\n",
    "            running_loss = total_train_loss / num_steps\n",
    "            pbar.set_postfix(loss=f'{running_loss:.4f}')\n",
    "\n",
    "        avg_train_loss = total_train_loss / num_steps\n",
    "\n",
    "        # === VALIDATION ===\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_steps = 0\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{args.epochs} Validation\")\n",
    "            for batch in pbar_val:\n",
    "                # Fast batch processing with cached SDF features\n",
    "                sdf_features_batch, grasp_batch, score_batch = process_batch_with_cached_features(\n",
    "                    batch, val_sdf_cache, device\n",
    "                )\n",
    "\n",
    "                actual_batch_size = sdf_features_batch.size(0)\n",
    "\n",
    "                # Concatenate and predict\n",
    "                flattened_features = torch.cat([sdf_features_batch, grasp_batch], dim=1)\n",
    "                pred_quality = model(flattened_features)\n",
    "                loss = criterion(pred_quality, score_batch)\n",
    "\n",
    "                total_val_loss += loss.item() * actual_batch_size\n",
    "                num_steps += actual_batch_size\n",
    "                \n",
    "                # Update progress bar\n",
    "                running_val_loss = total_val_loss / num_steps\n",
    "                pbar_val.set_postfix(val_loss=f'{running_val_loss:.4f}')\n",
    "\n",
    "        avg_val_loss = total_val_loss / num_steps\n",
    "        \n",
    "        # Clear SDF caches to free GPU memory\n",
    "        del train_sdf_cache, val_sdf_cache\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{args.epochs}], LR: {current_lr:.2e}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(avg_val_loss):\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
